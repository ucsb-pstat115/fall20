---
title: "Lab 2"
author: "PSTAT 115, Fall 2019"
date: "October 09, 2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
header-includes:
 \usepackage{float}
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=5,
                      fig.align='center',
                      fig.pos = 'H')
library(tidyverse)
library(ggplot2)
```



# Goal:

- Brief review: population and sample, likelihood, sufficient statistics.

- An example of data generating process (DGP).

- An example of Bayesian analysis.

# Review


```{r pop_samp, echo=FALSE, fig.cap="Population and Sample", out.width = '80%'}
knitr::include_graphics("images/population_sample.png")
```


```{r likelihood, echo=FALSE, fig.cap="Likelihood", out.width = '100%'}
knitr::include_graphics("images/likelihood.png")
```

```{r suff_stat, echo=FALSE, fig.cap="Sufficient Statistics", out.width = '100%'}
knitr::include_graphics("images/suff_stat.png")
```


# Data Generating Process (DGP)

## Dataset and Background
  If you ever want to just experiment with numbers, there are a bunch of handy datasets built into common R packages. To see them type:
  
```{r}
data()
```

Let's use one of these to talk about generating models + covariates.
 
ChickWeight is a dataset of weights collected over time for a number of different chicks on different diets. Let's load up the ChickWeight dataset into a tibble to make it easy to work with:

```{r}
df = ChickWeight %>% as.tibble
```

The ```%>%``` notation is a pipe in the tidyverse package. It is shorthand for:
  
```{r}
df = as.tibble(ChickWeight)
```

Using tibbles (instead of base R dataframes) and pipes (instead of function calls) can make it easier to do basic data transformations. You can do all of this in base R as well.

First let's print the ChickWeight tibble and see what's in it:

```{r}
df
```

Usually we'd look at the data before writing out a generating model. Just as an exercise though, let's do the reverse. We'll write out a generating model and then make some plots to see how reasonable our assumptions were.

* What is it that we're trying to model?

* What other data did we collect to explain what we're modeling (covariates)?

The simplest thing we might assume is that errors in our measurements are normally distributed. This means:

$$\text{weight} \sim N(\mu, \sigma)$$
In this case, weight is fixed data, and we'll need to estimate $\mu$ and $\sigma$ (they are the estimands).

* What might be a limitation of the normal assumption here?

How might we incorporate the chickens growing over time? We might assume that chickens grow linearly with time:

$$\text{weight}_i \sim N(\alpha t_i + \beta, \sigma)$$
In this case the new estimands are $\alpha$, $\beta$, and $\sigma$. $\text{weight}_i$ and $t_i$ are both data.

How might we expect the different diets to affect things? Maybe it doesn't affect the time zero weight of the chicken ($\beta$), but it does affect the slope ($\alpha$)? We can code in a diet dependence easily enough:

$$\text{weight}_{i, d} \sim N(\alpha_d t_i + \beta, \sigma)$$
The generating model also requires us to specify how the chickens are given a diet. We might assume this is uniformly sampled from the available diets, or something else. This is more important if we actually need to estimate the diet the chicken ate. In this case, the diet is given.

## Pseudocode

Assume that the diet is categorically distributed with 4 categories and corresponding probabilities $p = (p_1, p_2, p_3, p_4) = (0.4, 0.2, 0.2, 0.2)$. The covariate *time* and parameters $\alpha_1, \cdots, \alpha_k$, $\beta$ are given.

```{r pseudocode, eval = FALSE}
for(i in 1:nrow(ChickWeight)) {
  Draw diet_i from Categorical(4, p)
  Set alpha_i = alpha_k if diet_i = k, k = 1, 2, 3, 4
  Draw weight_i from Normal(alpha_i * t_i + beta, sigma)
}
```

## Exploring our data

Since it is the weights we are concerned with, let's look at that first.

```{r}
df %>%
  ggplot(aes(weight)) +
  geom_histogram()
```
Perhaps unsurprisingly, this plot is mostly unintelligible. There's all sorts of things mixed together, so let's try to use some more plots to break it apart.

Let's have a look at $p(\text{weight} | \text{Time} = t)$.

```{r}
df %>%
  filter(Time == 16) %>%
  ggplot(aes(weight)) +
  geom_histogram()
```

If we assume this conditional distribution is normal, then we're saying the output (weight), is characterized by two parameters $\mu$ and $\sigma$ (that can be functions of the covariates). $\mu$ and $\sigma$ are our estimands here -- the things we'd try to estimate.

Let's try to get an idea of what $\mu(t)$ would look like by plotting weight vs. time:

```{r}
df %>%
  ggplot(aes(Time, weight)) +
  geom_point()
```

* How does this differ from what we proposed initially?

Let's try to get a handle on how diet affects any of this:

```{r}
df %>%
  ggplot(aes(Time, weight)) +
  geom_point(aes(color = Diet))
```

That's kindof hard to see. Maybe we can break this out into different plots:

```{r}
df %>%
  ggplot(aes(Time, weight)) +
  geom_point(aes(color = Diet)) +
  facet_wrap(~ Diet, nrow = 2, labeller = label_both)
```

* Does diet seem to have a strong affect on the growth of the chickens?

* What might be the next step in this analysis?


# Bayesian Example

An example from BDA3 (the Gelman book in the syllabus) is a spelling correction problem. The problem setting is, assume that a user searched for "radom". The question is, what is the probability that they typed something else?

First assume that there are only three words to consider, "radom", "random", and "radon". Assume also that you've been given the likelihoods for typing "radom" given you actually wanted to type each of those three words and also the prior probability that each of those words was what was typed.

Likelihoods:

$$p(\text{typed radom} | \text{wanted to type random}) = 0.00193$$
$$p(\text{typed radom} | \text{wanted to type radon}) = 0.000143$$
$$p(\text{typed radom} | \text{wanted to type radom}) = 0.975$$

Priors:

$$p(\text{wanted to type random}) = 7.60 * 10^{-5}$$
$$p(\text{wanted to type radon}) = 6.05 * 10^{-6}$$
$$p(\text{wanted to type radom}) = 3.12 * 10^{-7}$$

Now we can use Bayes' rule to compute the posterior.

```{r}
likelihood = c(0.00193, 0.000143, 0.975)
prior = c(7.60e-5, 6.05e-6, 3.12e-7)

unnormalized_posterior = likelihood * prior
posterior = unnormalized_posterior / sum(unnormalized_posterior)
posterior
```

Posterior:

$$p(\text{wanted to type random} | \text{typed radom}) = 0.325$$
$$p(\text{wanted to type radon} | \text{typed radom}) = 0.002$$
$$p(\text{wanted to type radom} | \text{typed radom}) = 0.673$$

# Questions

- Was this what you expected?

- How do you think the prior was estimated?

- How do you think the likelihood was estimated?
























